<launch>
    <node pkg="yolov7_ros" type="detect_ros.py" name="detect" output="screen" ns="yolov7">
        <!-- Download the official weights from the original repo -->
        <param name="weights_path" type="str"
        value="/home/kong/my_ws/yolov7-ros-pose-estimation/src/yolov7-ros/data/yolov7-w6-pose.pt"/>
        <!-- Path to a class_labels.txt file containing your desired class labels. The i-th entry corresponds to the i-th class id. For example, in coco class label 0 corresponds to 'person'. Files for the coco and berkeley deep drive datasets are provided in the 'class_labels/' directory. If you leave it empty then no class labels are visualized.-->
        <param name="classes_path" type="str" value="/home/kong/my_ws/yolov7-ros-pose-estimation/src/yolov7-ros/class_labels/berkeley.txt" />
        <!-- topic name to subscribe to -->
        <param name="img_topic" type="str" value="/usb_cam/image_raw" />
        <!-- confidence threshold -->
        <param name="conf_thresh" type="double" value="0.35" />
        <!-- intersection over union threshold -->
        <param name="iou_thresh" type="double" value="0.45" />
        <!-- queue size for publishing -->
        <param name="queue_size" type="int" value="10" />
        <!-- image size to which to resize each input image before feeding into the
        network (the final output is rescaled to the original image size) -->
        <param name="img_size" type="int" value="640" />
        <!-- flag whether to also publish image with the visualized detections -->
        <param name="visualize" type="bool" value="true" />
        <!-- 'cuda' or 'cpu' -->
        <param name="device" type="str" value="cuda" />
    </node>

    <node pkg="action-estimator" type="action_estimator.py" name="action_estimator" output="screen" ns="action">
        <!-- topic name to subscribe to -->
        <param name="img_topic" type="str" value="/yolov7/image" />
        <!-- topic name to subscribe to -->
        <param name="pose_topic" type="str" value="/yolov7/kpt" />
        <!-- topic name for the detection output -->
        <param name="out_topic" type="str" value="action_est" />
        <!-- queue size for publishing -->
        <param name="queue_size" type="int" value="10" />
        <!-- flag whether to also publish image with the visualized detections -->
        <param name="visualize" type="bool" value="true" />
    </node>

    <!-- Use DeepSort to track humans and limb parts -->
    <node pkg="pose-tracker" type="pose_tracker.py" name="track" output="screen" ns="tracker">
        <param name="img_topic" type="str" value="/yolov7/image" />
        <param name="pose_topic" type="str" value="/yolov7/kpt" />
        <param name="out_topic" type="str" value="action_est" />
        <param name="queue_size" type="int" value="10" />
        <param name="visualize" type="bool" value="true" />
    </node>

    <!-- input data -->
   <!-- launch video stream -->
    <include file="$(find video_stream_opencv)/launch/camera.launch">
        <arg name="camera_name" value="usb_cam"/>
        <arg name="video_stream_provider" value="$(find video_stream_opencv)/video/football1.mp4"/>
        <arg name="buffer_queue_size" value="1000"/>
        <arg name="fps" value="30"/>
        <arg name="frame_id" value="videofile_frame"/>
        <arg name="camera_info_url" value=""/>
        <arg name="flip_horizontal" value="false"/>
        <arg name="flip_vertical" value="false"/>
        <arg name="loop_videofile" value="true"/>
        <arg name="start_frame" value="30"/>
        <arg name="stop_frame" value="150"/>
        <arg name="visualize" value="false"/>
    </include>

</launch>
